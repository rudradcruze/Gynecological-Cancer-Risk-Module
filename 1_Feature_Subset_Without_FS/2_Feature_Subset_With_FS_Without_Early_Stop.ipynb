{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7003eb9bca2a27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T03:49:07.142238Z",
     "start_time": "2025-06-03T03:49:07.138933Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (roc_curve, auc, precision_score, f1_score, \n",
    "                           matthews_corrcoef, accuracy_score, recall_score,\n",
    "                           confusion_matrix)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "import gc\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52316e97a39cf028",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T03:49:07.151257Z",
     "start_time": "2025-06-03T03:49:07.149118Z"
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set font family globally\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "dpi = 1000\n",
    "plt.rcParams['figure.dpi'] = dpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687eb22ba2fd98e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T03:49:07.160142Z",
     "start_time": "2025-06-03T03:49:07.157115Z"
    }
   },
   "outputs": [],
   "source": [
    "# Base directory\n",
    "BASE_DIR = \"AttBiLSTM_Analysis_With_FS_Without_Early_Stop\"\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_DIR, 'models'), exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_DIR, 'plots'), exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_DIR, 'results'), exist_ok=True)\n",
    "\n",
    "# Dataset sizes to compare\n",
    "DATASET_SIZES = [6000, 7000, 8000, 9000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4876443c910be786",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T03:49:07.194751Z",
     "start_time": "2025-06-03T03:49:07.169077Z"
    }
   },
   "outputs": [],
   "source": [
    "# Device setup\n",
    "device = torch.device(\"cpu\")\n",
    "try:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"MPS device detected, using MPS\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"CUDA device detected, using CUDA\")\n",
    "    print(f\"Using device: {device}\")\n",
    "except:\n",
    "    print(\"Error detecting device capabilities, defaulting to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1695605ebe7f3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T03:49:07.714798Z",
     "start_time": "2025-06-03T03:49:07.200494Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "print(\"Loading and preparing data...\")\n",
    "try:\n",
    "    data = pd.read_csv('../dataset/Combined_Common_Genes_With_Target_ML_Selected_Top_9000_Features.csv')\n",
    "    print(f\"Full dataset info:\")\n",
    "    print(f\"Shape: {data.shape}\")\n",
    "    print(f\"Target column: {data.columns[-1]}\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X_full = data.iloc[:, :-1].values  # All columns except last\n",
    "    y_full = data.iloc[:, -1].values   # Last column (target)\n",
    "    \n",
    "    print(f\"Full features shape: {X_full.shape}\")\n",
    "    print(f\"Full target shape: {y_full.shape}\")\n",
    "    \n",
    "    # Check target distribution\n",
    "    unique_targets, counts = np.unique(y_full, return_counts=True)\n",
    "    print(f\"Target distribution:\")\n",
    "    for target, count in zip(unique_targets, counts):\n",
    "        print(f\"  Class {target}: {count} samples ({count/len(y_full)*100:.2f}%)\")\n",
    "    \n",
    "    num_classes = len(unique_targets)\n",
    "    num_features = X_full.shape[1]\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744ac249fe7be8ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T03:49:07.732427Z",
     "start_time": "2025-06-03T03:49:07.729453Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom activation functions\n",
    "class Mish(nn.Module):\n",
    "    \"\"\"Mish activation function: x * tanh(softplus(x))\"\"\"\n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(F.softplus(x))\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    \"\"\"Swish activation function: x * sigmoid(x)\"\"\"\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "# Register custom activations\n",
    "nn.Mish = Mish\n",
    "nn.Swish = Swish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf8c1f613d41d56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T03:49:07.744932Z",
     "start_time": "2025-06-03T03:49:07.740959Z"
    }
   },
   "outputs": [],
   "source": [
    "# Advanced Attention Mechanism\n",
    "class MultiScaleAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads=4):\n",
    "        super(MultiScaleAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        # Multiple attention heads with different perspectives\n",
    "        self.query_nets = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, self.head_dim) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.key_nets = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, self.head_dim) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.value_nets = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, self.head_dim) for _ in range(num_heads)\n",
    "        ])\n",
    "        \n",
    "        self.output_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Multi-head attention\n",
    "        attention_outputs = []\n",
    "        attention_weights_list = []\n",
    "        \n",
    "        for i in range(self.num_heads):\n",
    "            query = self.query_nets[i](x)  # [batch, seq_len, head_dim]\n",
    "            key = self.key_nets[i](x)\n",
    "            value = self.value_nets[i](x)\n",
    "            \n",
    "            # Compute attention scores\n",
    "            scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "            attention_weights = F.softmax(scores, dim=-1)\n",
    "            attention_weights_list.append(attention_weights)\n",
    "            \n",
    "            # Apply attention to values\n",
    "            attended = torch.matmul(attention_weights, value)\n",
    "            attention_outputs.append(attended)\n",
    "        \n",
    "        # Concatenate all heads\n",
    "        multi_head_output = torch.cat(attention_outputs, dim=-1)\n",
    "        \n",
    "        # Project and apply residual connection\n",
    "        output = self.output_proj(multi_head_output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.layer_norm(x + output)  # Residual connection\n",
    "        \n",
    "        return output, attention_weights_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea42efa280709b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T03:49:07.763818Z",
     "start_time": "2025-06-03T03:49:07.758638Z"
    }
   },
   "outputs": [],
   "source": [
    "# Advanced AttBiLSTM Model\n",
    "class AdvancedAttBiLSTM(nn.Module):\n",
    "    \"\"\"Advanced Attention-based Bidirectional LSTM with custom activation functions\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, num_classes, hidden_dim=128, num_layers=2, dropout=0.3):\n",
    "        super(AdvancedAttBiLSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Input projection layer\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),  # GELU activation instead of ReLU\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Bidirectional LSTM layers\n",
    "        self.lstm_layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            input_size = hidden_dim if i == 0 else hidden_dim * 2\n",
    "            self.lstm_layers.append(\n",
    "                nn.LSTM(\n",
    "                    input_size=input_size,\n",
    "                    hidden_size=hidden_dim,\n",
    "                    num_layers=1,\n",
    "                    bidirectional=True,\n",
    "                    batch_first=True,\n",
    "                    dropout=dropout if i < num_layers - 1 else 0\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Multi-scale attention mechanism\n",
    "        self.attention = MultiScaleAttention(hidden_dim * 2, num_heads=4)\n",
    "        \n",
    "        # Feature fusion layers\n",
    "        self.feature_fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Mish(),  # Mish activation function\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.Swish(),  # Swish activation function\n",
    "            nn.Dropout(dropout // 2)\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.BatchNorm1d(hidden_dim // 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout // 2),\n",
    "            nn.Linear(hidden_dim // 4, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize model weights using Xavier/He initialization\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.LSTM):\n",
    "                for name, param in module.named_parameters():\n",
    "                    if 'weight_ih' in name:\n",
    "                        nn.init.xavier_uniform_(param.data)\n",
    "                    elif 'weight_hh' in name:\n",
    "                        nn.init.orthogonal_(param.data)\n",
    "                    elif 'bias' in name:\n",
    "                        nn.init.zeros_(param.data)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Project input features\n",
    "        x = self.input_projection(x)  # [batch_size, hidden_dim]\n",
    "        x = x.unsqueeze(1)  # [batch_size, 1, hidden_dim] - create sequence dimension\n",
    "        \n",
    "        # Apply LSTM layers\n",
    "        for lstm in self.lstm_layers:\n",
    "            x, _ = lstm(x)  # [batch_size, seq_len, hidden_dim * 2]\n",
    "        \n",
    "        # Apply multi-scale attention\n",
    "        x_attended, attention_weights = self.attention(x)  # [batch_size, seq_len, hidden_dim * 2]\n",
    "        \n",
    "        # Global average pooling and max pooling\n",
    "        avg_pool = torch.mean(x_attended, dim=1)  # [batch_size, hidden_dim * 2]\n",
    "        max_pool, _ = torch.max(x_attended, dim=1)  # [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        # Combine pooled features\n",
    "        combined_features = avg_pool + max_pool  # Element-wise addition\n",
    "        \n",
    "        # Feature fusion\n",
    "        fused_features = self.feature_fusion(combined_features)\n",
    "        \n",
    "        # Final classification\n",
    "        output = self.classifier(fused_features)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8385fb67703fa59c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T03:49:07.768642Z",
     "start_time": "2025-06-03T03:49:07.766022Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_dataset_by_size(X_full, y_full, dataset_size):\n",
    "    \"\"\"Prepare dataset for specific size\"\"\"\n",
    "    print(f\"\\nPreparing dataset with {dataset_size} samples...\")\n",
    "    \n",
    "    # Take first n samples\n",
    "    X = X_full[:dataset_size]\n",
    "    y = y_full[:dataset_size]\n",
    "    \n",
    "    print(f\"Dataset size: {X.shape}\")\n",
    "    \n",
    "    # Check target distribution for this subset\n",
    "    unique_targets, counts = np.unique(y, return_counts=True)\n",
    "    print(f\"Target distribution for {dataset_size} samples:\")\n",
    "    for target, count in zip(unique_targets, counts):\n",
    "        print(f\"  Class {target}: {count} samples ({count/len(y)*100:.2f}%)\")\n",
    "    \n",
    "    # Data preprocessing and scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Split data (70% train, 15% val, 15% test)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01ad3fe5ffc0c03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T03:49:07.778440Z",
     "start_time": "2025-06-03T03:49:07.776045Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_data_loaders(X_train, y_train, X_val, y_val, batch_size=32):\n",
    "    \"\"\"Create PyTorch data loaders\"\"\"\n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.LongTensor(y_train)\n",
    "    X_val_tensor = torch.FloatTensor(X_val)\n",
    "    y_val_tensor = torch.LongTensor(y_val)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ac6007040a3e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T03:49:07.798919Z",
     "start_time": "2025-06-03T03:49:07.792913Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, y_train, epochs=200):\n",
    "    \"\"\"Train the AttBiLSTM model without early stopping\"\"\"\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # Calculate class weights for imbalanced data\n",
    "    class_counts = np.bincount(y_train)\n",
    "    class_weights = torch.FloatTensor([len(y_train) / (len(class_counts) * count) \n",
    "                                     for count in class_counts]).to(device)\n",
    "    \n",
    "    # Loss function with class weights\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    # Optimizer with different learning rates for different parts\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': model.input_projection.parameters(), 'lr': 1e-3},\n",
    "        {'params': model.lstm_layers.parameters(), 'lr': 5e-4},\n",
    "        {'params': model.attention.parameters(), 'lr': 1e-3},\n",
    "        {'params': model.feature_fusion.parameters(), 'lr': 1e-3},\n",
    "        {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
    "    ], weight_decay=1e-4)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, verbose=False\n",
    "    )\n",
    "    \n",
    "    # Mixed precision training\n",
    "    use_amp = device.type == \"cuda\"\n",
    "    scaler = GradScaler() if use_amp else None\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': []\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    output, _ = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                output, _ = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            train_correct += pred.eq(target).sum().item()\n",
    "            train_total += target.size(0)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output, _ = model(data)\n",
    "                val_loss += criterion(output, target).item()\n",
    "                pred = output.argmax(dim=1)\n",
    "                val_correct += pred.eq(target).sum().item()\n",
    "                val_total += target.size(0)\n",
    "        \n",
    "        # Calculate average losses and accuracies\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, '\n",
    "                  f'Val Loss: {avg_val_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
    "                  f'Val Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    return model, history, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cd3b071032cf49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T03:49:07.807989Z",
     "start_time": "2025-06-03T03:49:07.801797Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_detailed_metrics(y_true, y_pred, y_pred_proba):\n",
    "    \"\"\"Calculate comprehensive metrics including TP, TN, FP, FN, TPR, TNR, FPR, FNR\"\"\"\n",
    "    \n",
    "    # Basic metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # For binary classification\n",
    "    if len(np.unique(y_true)) == 2:\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        \n",
    "        # Rates\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # Sensitivity/Recall\n",
    "        tnr = tn / (tn + fp) if (tn + fp) > 0 else 0  # Specificity\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # Fall-out\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0  # Miss rate\n",
    "        \n",
    "        # AUC and ROC data\n",
    "        fpr_roc, tpr_roc, _ = roc_curve(y_true, y_pred_proba[:, 1])\n",
    "        auc_score = auc(fpr_roc, tpr_roc)\n",
    "        roc_data = (fpr_roc, tpr_roc, auc_score)\n",
    "        \n",
    "        detailed_metrics = {\n",
    "            'ACC': acc,\n",
    "            'AUC': auc_score,\n",
    "            'PRE': precision,\n",
    "            'SP': tnr,  # Specificity\n",
    "            'SN': tpr,  # Sensitivity\n",
    "            'F1': f1,\n",
    "            'MCC': mcc,\n",
    "            'TPR': tpr,\n",
    "            'FPR': fpr,\n",
    "            'TNR': tnr,\n",
    "            'FNR': fnr,\n",
    "            'TP': int(tp),\n",
    "            'TN': int(tn),\n",
    "            'FP': int(fp),\n",
    "            'FN': int(fn)\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        # For multiclass classification\n",
    "        # Calculate macro-averaged rates\n",
    "        tpr_list, fpr_list, tnr_list, fnr_list = [], [], [], []\n",
    "        tp_total, tn_total, fp_total, fn_total = 0, 0, 0, 0\n",
    "        \n",
    "        auc_scores = []\n",
    "        \n",
    "        for i in range(len(np.unique(y_true))):\n",
    "            # One-vs-Rest for each class\n",
    "            y_true_binary = (y_true == i).astype(int)\n",
    "            y_pred_binary = (y_pred == i).astype(int)\n",
    "            \n",
    "            tn_i = np.sum((y_true_binary == 0) & (y_pred_binary == 0))\n",
    "            fp_i = np.sum((y_true_binary == 0) & (y_pred_binary == 1))\n",
    "            fn_i = np.sum((y_true_binary == 1) & (y_pred_binary == 0))\n",
    "            tp_i = np.sum((y_true_binary == 1) & (y_pred_binary == 1))\n",
    "            \n",
    "            tp_total += tp_i\n",
    "            tn_total += tn_i\n",
    "            fp_total += fp_i\n",
    "            fn_total += fn_i\n",
    "            \n",
    "            tpr_i = tp_i / (tp_i + fn_i) if (tp_i + fn_i) > 0 else 0\n",
    "            tnr_i = tn_i / (tn_i + fp_i) if (tn_i + fp_i) > 0 else 0\n",
    "            fpr_i = fp_i / (fp_i + tn_i) if (fp_i + tn_i) > 0 else 0\n",
    "            fnr_i = fn_i / (fn_i + tp_i) if (fn_i + tp_i) > 0 else 0\n",
    "            \n",
    "            tpr_list.append(tpr_i)\n",
    "            tnr_list.append(tnr_i)\n",
    "            fpr_list.append(fpr_i)\n",
    "            fnr_list.append(fnr_i)\n",
    "            \n",
    "            # ROC curve for each class\n",
    "            fpr_roc, tpr_roc, _ = roc_curve(y_true_binary, y_pred_proba[:, i])\n",
    "            auc_i = auc(fpr_roc, tpr_roc)\n",
    "            auc_scores.append(auc_i)\n",
    "        \n",
    "        # For multiclass, create averaged ROC curve\n",
    "        fpr_avg = np.linspace(0, 1, 100)\n",
    "        tpr_avg = np.zeros_like(fpr_avg)\n",
    "        for i in range(len(np.unique(y_true))):\n",
    "            y_true_binary = (y_true == i).astype(int)\n",
    "            fpr_i, tpr_i, _ = roc_curve(y_true_binary, y_pred_proba[:, i])\n",
    "            tpr_interp = np.interp(fpr_avg, fpr_i, tpr_i)\n",
    "            tpr_avg += tpr_interp\n",
    "        tpr_avg /= len(np.unique(y_true))\n",
    "        auc_avg = auc(fpr_avg, tpr_avg)\n",
    "        roc_data = (fpr_avg, tpr_avg, auc_avg)\n",
    "        \n",
    "        detailed_metrics = {\n",
    "            'ACC': acc,\n",
    "            'AUC': np.mean(auc_scores),\n",
    "            'PRE': precision,\n",
    "            'SP': np.mean(tnr_list),  # Specificity\n",
    "            'SN': np.mean(tpr_list),  # Sensitivity\n",
    "            'F1': f1,\n",
    "            'MCC': mcc,\n",
    "            'TPR': np.mean(tpr_list),\n",
    "            'FPR': np.mean(fpr_list),\n",
    "            'TNR': np.mean(tnr_list),\n",
    "            'FNR': np.mean(fnr_list),\n",
    "            'TP': int(tp_total),\n",
    "            'TN': int(tn_total),\n",
    "            'FP': int(fp_total),\n",
    "            'FN': int(fn_total)\n",
    "        }\n",
    "    \n",
    "    return detailed_metrics, roc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32f86042b60791d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T03:49:07.819360Z",
     "start_time": "2025-06-03T03:49:07.815910Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_combined_roc_curves(all_test_data):\n",
    "    \"\"\"Plot combined ROC curves for all dataset sizes\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['blue', 'red', 'green', 'purple']\n",
    "    \n",
    "    for i, (size, roc_data) in enumerate(all_test_data.items()):\n",
    "        if roc_data is not None:\n",
    "            fpr, tpr, auc_score = roc_data\n",
    "            plt.plot(fpr, tpr, color=colors[i], linewidth=3, \n",
    "                    label=f'{size} samples (AUC = {auc_score:.4f})')\n",
    "    \n",
    "    # Diagonal line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=2, alpha=0.7)\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=24)\n",
    "    plt.ylabel('True Positive Rate', fontsize=24)\n",
    "    plt.title('ROC Curves Comparison Across Dataset Sizes', fontsize=28, pad=20)\n",
    "    plt.legend(loc=\"lower right\", fontsize=20)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.grid(False)\n",
    "    \n",
    "    # Enhance aesthetics\n",
    "    ax = plt.gca()\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(2)\n",
    "        spine.set_color('black')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(BASE_DIR, 'plots', 'combined_roc_curves.png'), dpi=dpi, bbox_inches='tight')\n",
    "    plt.savefig(os.path.join(BASE_DIR, 'plots', 'combined_roc_curves.pdf'), dpi=dpi, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a792da3a2d2715e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T03:49:07.830829Z",
     "start_time": "2025-06-03T03:49:07.826926Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_separate_training_validation_loss(all_histories):\n",
    "    \"\"\"Plot separate training and validation loss curves\"\"\"\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'purple']\n",
    "    \n",
    "    # 1. Training Loss Comparison\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, size in enumerate(DATASET_SIZES):\n",
    "        plt.plot(all_histories[size]['train_loss'], color=colors[i], \n",
    "                linewidth=2, label=f'{size} samples')\n",
    "    plt.title('Training Loss Comparison of AttBiLSTM With FS', fontsize=28, pad=20)\n",
    "    plt.xlabel('Epoch', fontsize=24)\n",
    "    plt.ylabel('Loss', fontsize=24)\n",
    "    plt.legend(fontsize=20)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.grid(False)\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(2)\n",
    "        spine.set_color('black')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(BASE_DIR, 'plots', 'training_loss_comparison.png'), dpi=dpi, bbox_inches='tight')\n",
    "    plt.savefig(os.path.join(BASE_DIR, 'plots', 'training_loss_comparison.pdf'), dpi=dpi, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Validation Loss Comparison\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, size in enumerate(DATASET_SIZES):\n",
    "        plt.plot(all_histories[size]['val_loss'], color=colors[i], \n",
    "                linewidth=2, label=f'{size} samples')\n",
    "    plt.title('Validation Loss Comparison of AttBiLSTM With FS', fontsize=28, pad=20)\n",
    "    plt.xlabel('Epoch', fontsize=24)\n",
    "    plt.ylabel('Loss', fontsize=24)\n",
    "    plt.legend(fontsize=20)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.grid(False)\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(2)\n",
    "        spine.set_color('black')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(BASE_DIR, 'plots', 'validation_loss_comparison.png'), dpi=dpi, bbox_inches='tight')\n",
    "    plt.savefig(os.path.join(BASE_DIR, 'plots', 'validation_loss_comparison.pdf'), dpi=dpi, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9760bada69566a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T03:49:07.849228Z",
     "start_time": "2025-06-03T03:49:07.839367Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_performance_metrics_variants(all_metrics):\n",
    "    \"\"\"Create different types of performance comparison graphs\"\"\"\n",
    "    \n",
    "    metrics_to_plot = ['ACC', 'AUC', 'PRE', 'SN', 'SP', 'F1', 'MCC']\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2']\n",
    "    \n",
    "    # 1. Classic Bar Chart\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    x_pos = np.arange(len(DATASET_SIZES))\n",
    "    \n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        values = [all_metrics[size][metric] for size in DATASET_SIZES]\n",
    "        bars = plt.bar(x_pos, values, color=colors[i], alpha=0.7, edgecolor='black', linewidth=1)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for j, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                    f'{height:.3f}', ha='center', va='bottom', fontsize=20)\n",
    "        \n",
    "        plt.title(f'{metric}', fontsize=28, fontweight='bold')\n",
    "        plt.xlabel('Dataset Size', fontsize=24)\n",
    "        plt.ylabel(metric, fontsize=24)\n",
    "        plt.xticks(x_pos, DATASET_SIZES, fontsize=18)\n",
    "        plt.yticks(fontsize=18)\n",
    "        plt.ylim(0, max(values) * 1.1)\n",
    "        plt.grid(False)\n",
    "        \n",
    "        # Enhance aesthetics\n",
    "        ax = plt.gca()\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_linewidth(1.5)\n",
    "            spine.set_color('black')\n",
    "    \n",
    "    # Remove empty subplot\n",
    "    plt.subplot(2, 4, 8)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.suptitle('Performance Metrics Comparison (Bar Chart)', fontsize=32, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(BASE_DIR, 'plots', 'metrics_comparison_bar.png'), dpi=dpi, bbox_inches='tight')\n",
    "    plt.savefig(os.path.join(BASE_DIR, 'plots', 'metrics_comparison_bar.pdf'), dpi=dpi, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Line Graph with Markers\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        values = [all_metrics[size][metric] for size in DATASET_SIZES]\n",
    "        plt.plot(DATASET_SIZES, values, 'o-', linewidth=3, markersize=8, \n",
    "                color=colors[i], markerfacecolor='white', markeredgewidth=2)\n",
    "        \n",
    "        # Add value annotations\n",
    "        for j, (x, y) in enumerate(zip(DATASET_SIZES, values)):\n",
    "            plt.annotate(f'{y:.3f}', (x, y), textcoords=\"offset points\", \n",
    "                        xytext=(0,10), ha='center', fontsize=18)\n",
    "        \n",
    "        plt.title(f'{metric}', fontsize=28, fontweight='bold')\n",
    "        plt.xlabel('Dataset Size', fontsize=24)\n",
    "        plt.ylabel(metric, fontsize=24)\n",
    "        plt.xticks(DATASET_SIZES, fontsize=18)\n",
    "        plt.yticks(fontsize=18)\n",
    "        plt.grid(False)\n",
    "        \n",
    "        # Enhance aesthetics\n",
    "        ax = plt.gca()\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_linewidth(1.5)\n",
    "            spine.set_color('black')\n",
    "    \n",
    "    plt.subplot(2, 4, 8)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.suptitle('Performance Metrics Comparison (Line Graph)', fontsize=32, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(BASE_DIR, 'plots', 'metrics_comparison_line.png'), dpi=dpi, bbox_inches='tight')\n",
    "    plt.savefig(os.path.join(BASE_DIR, 'plots', 'metrics_comparison_line.pdf'), dpi=dpi, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Violin Plot\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        # Create data for violin plot (simulate some distribution around the actual values)\n",
    "        violin_data = []\n",
    "        for size in DATASET_SIZES:\n",
    "            base_value = all_metrics[size][metric]\n",
    "            # Add some synthetic variation for visualization\n",
    "            variation = np.random.normal(base_value, base_value * 0.01, 50)\n",
    "            violin_data.append(variation)\n",
    "        \n",
    "        parts = axes[i].violinplot(violin_data, positions=range(len(DATASET_SIZES)), \n",
    "                                   showmeans=True, showmedians=True)\n",
    "        \n",
    "        # Color the violin plots\n",
    "        for pc in parts['bodies']:\n",
    "            pc.set_facecolor(colors[i])\n",
    "            pc.set_alpha(0.7)\n",
    "        \n",
    "        axes[i].set_title(f'{metric}', fontsize=28, fontweight='bold')\n",
    "        axes[i].set_xlabel('Dataset Size', fontsize=24)\n",
    "        axes[i].set_ylabel(metric, fontsize=24)\n",
    "        axes[i].set_xticks(range(len(DATASET_SIZES)))\n",
    "        axes[i].set_xticklabels(DATASET_SIZES, fontsize=18)\n",
    "        axes[i].tick_params(axis='y', labelsize=18)\n",
    "        axes[i].grid(False)\n",
    "        \n",
    "        # Enhance aesthetics\n",
    "        for spine in axes[i].spines.values():\n",
    "            spine.set_linewidth(1.5)\n",
    "            spine.set_color('black')\n",
    "    \n",
    "    # Remove empty subplot\n",
    "    axes[7].axis('off')\n",
    "    \n",
    "    plt.suptitle('Performance Metrics Comparison (Violin Plot)', fontsize=32, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(BASE_DIR, 'plots', 'metrics_comparison_violin.png'), dpi=dpi, bbox_inches='tight')\n",
    "    plt.savefig(os.path.join(BASE_DIR, 'plots', 'metrics_comparison_violin.pdf'), dpi=dpi, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Radar Chart\n",
    "    fig, ax = plt.subplots(figsize=(12, 12), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    # Number of metrics\n",
    "    N = len(metrics_to_plot)\n",
    "    \n",
    "    # Compute angle for each metric\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    \n",
    "    # Plot for each dataset size\n",
    "    for i, size in enumerate(DATASET_SIZES):\n",
    "        values = [all_metrics[size][metric] for metric in metrics_to_plot]\n",
    "        values += values[:1]  # Complete the circle\n",
    "        \n",
    "        ax.plot(angles, values, 'o-', linewidth=3, label=f'{size} samples', \n",
    "                color=colors[i % len(colors)], markersize=8)\n",
    "        ax.fill(angles, values, alpha=0.25, color=colors[i % len(colors)])\n",
    "    \n",
    "    # Add labels\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metrics_to_plot, fontsize=20)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=16)\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.title('Performance Metrics Comparison (Radar Chart)', \n",
    "              fontsize=28, fontweight='bold', pad=30)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0), fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(BASE_DIR, 'plots', 'metrics_comparison_radar.png'), dpi=dpi, bbox_inches='tight')\n",
    "    plt.savefig(os.path.join(BASE_DIR, 'plots', 'metrics_comparison_radar.pdf'), dpi=dpi, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f94f818782fc60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T03:49:07.862523Z",
     "start_time": "2025-06-03T03:49:07.857623Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_horizontal_time_comparison(all_metrics):\n",
    "    \"\"\"Plot horizontal bar charts for training and testing time\"\"\"\n",
    "    \n",
    "    # 1. Training Time Horizontal Bar\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    training_times = [all_metrics[size]['Training_Time'] for size in DATASET_SIZES]\n",
    "    y_pos = np.arange(len(DATASET_SIZES))\n",
    "    \n",
    "    bars = plt.barh(y_pos, training_times, color='skyblue', alpha=0.7, \n",
    "                    edgecolor='black', linewidth=2)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + max(training_times)*0.01, bar.get_y() + bar.get_height()/2,\n",
    "                f'{width:.1f}s', ha='left', va='center', fontsize=20)\n",
    "    \n",
    "    plt.title('Training Time Comparison of AttBiLSTM With FS', fontsize=28, pad=20)\n",
    "    plt.xlabel('Training Time (seconds)', fontsize=24)\n",
    "    plt.ylabel('Dataset Size', fontsize=24)\n",
    "    plt.yticks(y_pos, [f'{size} samples' for size in DATASET_SIZES], fontsize=18)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.grid(False)\n",
    "    \n",
    "    # Enhance aesthetics\n",
    "    ax = plt.gca()\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(2)\n",
    "        spine.set_color('black')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(BASE_DIR, 'plots', 'training_time_horizontal.png'), dpi=dpi, bbox_inches='tight')\n",
    "    plt.savefig(os.path.join(BASE_DIR, 'plots', 'training_time_horizontal.pdf'), dpi=dpi, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Testing Time Horizontal Bar\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    testing_times = [all_metrics[size]['Testing_Time'] for size in DATASET_SIZES]\n",
    "    \n",
    "    bars = plt.barh(y_pos, testing_times, color='lightcoral', alpha=0.7, \n",
    "                    edgecolor='black', linewidth=2)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + max(testing_times)*0.01, bar.get_y() + bar.get_height()/2,\n",
    "                f'{width:.3f}s', ha='left', va='center', fontsize=20)\n",
    "    \n",
    "    plt.title('Testing Time Comparison of AttBiLSTM With FS', fontsize=28, pad=20)\n",
    "    plt.xlabel('Testing Time (seconds)', fontsize=24)\n",
    "    plt.ylabel('Dataset Size', fontsize=24)\n",
    "    plt.yticks(y_pos, [f'{size} samples' for size in DATASET_SIZES], fontsize=18)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.grid(False)\n",
    "    \n",
    "    # Enhance aesthetics\n",
    "    ax = plt.gca()\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(2)\n",
    "        spine.set_color('black')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(BASE_DIR, 'plots', 'testing_time_horizontal.png'), dpi=dpi, bbox_inches='tight')\n",
    "    plt.savefig(os.path.join(BASE_DIR, 'plots', 'testing_time_horizontal.pdf'), dpi=dpi, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bef4a0ea9d1309a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T03:49:07.872671Z",
     "start_time": "2025-06-03T03:49:07.870315Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_comparative_results(all_metrics, all_histories, all_test_data):\n",
    "    \"\"\"Create comprehensive comparative plots for all dataset sizes\"\"\"\n",
    "    \n",
    "    print(\"\\nGenerating comprehensive comparative plots...\")\n",
    "    \n",
    "    # 1. Combined ROC Curves\n",
    "    plot_combined_roc_curves(all_test_data)\n",
    "    print(\"✓ Combined ROC curves saved\")\n",
    "    \n",
    "    # 2. Separate Training and Validation Loss\n",
    "    plot_separate_training_validation_loss(all_histories)\n",
    "    print(\"✓ Separate training/validation loss curves saved\")\n",
    "    \n",
    "    # 3. Performance Metrics Variants\n",
    "    plot_performance_metrics_variants(all_metrics)\n",
    "    print(\"✓ Performance metrics variants (bar, line, violin, radar) saved\")\n",
    "    \n",
    "    # 4. Horizontal Time Comparison\n",
    "    plot_horizontal_time_comparison(all_metrics)\n",
    "    print(\"✓ Horizontal time comparison charts saved\")\n",
    "    \n",
    "    print(\"All comparative plots saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6fc562e9147793",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T03:49:07.885490Z",
     "start_time": "2025-06-03T03:49:07.880362Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_analysis_for_dataset_size(dataset_size):\n",
    "    \"\"\"Run complete analysis for a specific dataset size\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RUNNING ANALYSIS FOR {dataset_size} SAMPLES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Prepare dataset\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, scaler = prepare_dataset_by_size(\n",
    "        X_full, y_full, dataset_size\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader, val_loader = create_data_loaders(X_train, y_train, X_val, y_val, batch_size=32)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = AdvancedAttBiLSTM(\n",
    "        input_dim=num_features,\n",
    "        num_classes=num_classes,\n",
    "        hidden_dim=128,\n",
    "        num_layers=2,\n",
    "        dropout=0.3\n",
    "    )\n",
    "    \n",
    "    print(f\"Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    trained_model, history, training_time = train_model(\n",
    "        model, train_loader, val_loader, y_train, epochs=200\n",
    "    )\n",
    "    \n",
    "    # Save model directly to main models folder\n",
    "    model_save_path = os.path.join(BASE_DIR, 'models', f'attbilstm_model_{dataset_size}.pth')\n",
    "    torch.save({\n",
    "        'model_state_dict': trained_model.state_dict(),\n",
    "        'model_config': {\n",
    "            'input_dim': num_features,\n",
    "            'num_classes': num_classes,\n",
    "            'hidden_dim': 128,\n",
    "            'num_layers': 2,\n",
    "            'dropout': 0.3\n",
    "        },\n",
    "        'training_history': history,\n",
    "        'dataset_size': dataset_size,\n",
    "        'scaler_params': {\n",
    "            'mean_': scaler.mean_.tolist(),\n",
    "            'scale_': scaler.scale_.tolist()\n",
    "        }\n",
    "    }, model_save_path)\n",
    "    print(f\"Model saved: {model_save_path}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"Evaluating on test set...\")\n",
    "    trained_model.eval()\n",
    "    X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        test_outputs, attention_weights = trained_model(X_test_tensor)\n",
    "        test_probabilities = F.softmax(test_outputs, dim=1).cpu().numpy()\n",
    "        test_predictions = np.argmax(test_probabilities, axis=1)\n",
    "    testing_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate detailed metrics and get ROC data\n",
    "    detailed_metrics, roc_data = calculate_detailed_metrics(y_test, test_predictions, test_probabilities)\n",
    "    \n",
    "    # Add additional information\n",
    "    detailed_metrics['Dataset_Size'] = dataset_size\n",
    "    detailed_metrics['Training_Time'] = training_time\n",
    "    detailed_metrics['Testing_Time'] = testing_time\n",
    "    detailed_metrics['Train_Samples'] = len(X_train)\n",
    "    detailed_metrics['Val_Samples'] = len(X_val)\n",
    "    detailed_metrics['Test_Samples'] = len(X_test)\n",
    "    \n",
    "    # Save results directly to main results folder\n",
    "    results_df = pd.DataFrame([detailed_metrics])\n",
    "    results_path = os.path.join(BASE_DIR, 'results', f'metrics_{dataset_size}.csv')\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    print(f\"Results saved: {results_path}\")\n",
    "    \n",
    "    # Save training history directly to main results folder\n",
    "    history_df = pd.DataFrame(history)\n",
    "    history_path = os.path.join(BASE_DIR, 'results', f'training_history_{dataset_size}.csv')\n",
    "    history_df.to_csv(history_path, index=False)\n",
    "    print(f\"History saved: {history_path}\")\n",
    "    \n",
    "    print(f\"Analysis completed for {dataset_size} samples\")\n",
    "    print(f\"Best metrics: ACC={detailed_metrics['ACC']:.4f}, AUC={detailed_metrics['AUC']:.4f}, F1={detailed_metrics['F1']:.4f}\")\n",
    "    \n",
    "    return detailed_metrics, history, roc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61df0c8239e9ea12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T03:49:07.900523Z",
     "start_time": "2025-06-03T03:49:07.893002Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_comprehensive_summary(all_metrics):\n",
    "    \"\"\"Create comprehensive summary report\"\"\"\n",
    "    \n",
    "    print(\"\\nGenerating comprehensive summary...\")\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for size in DATASET_SIZES:\n",
    "        metrics = all_metrics[size]\n",
    "        summary_data.append({\n",
    "            'Dataset_Size': size,\n",
    "            'Train_Samples': metrics['Train_Samples'],\n",
    "            'Val_Samples': metrics['Val_Samples'],\n",
    "            'Test_Samples': metrics['Test_Samples'],\n",
    "            'ACC': metrics['ACC'],\n",
    "            'AUC': metrics['AUC'],\n",
    "            'PRE': metrics['PRE'],\n",
    "            'SN': metrics['SN'],\n",
    "            'SP': metrics['SP'],\n",
    "            'F1': metrics['F1'],\n",
    "            'MCC': metrics['MCC'],\n",
    "            'Training_Time': metrics['Training_Time'],\n",
    "            'Testing_Time': metrics['Testing_Time']\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Save summary\n",
    "    summary_path = os.path.join(BASE_DIR, 'results', 'comprehensive_summary.csv')\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    \n",
    "    # Create detailed text report\n",
    "    report_path = os.path.join(BASE_DIR, 'results', 'analysis_report.txt')\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(\"AttBiLSTM Dataset Size Comparison Analysis Report (WITH FEATURE SELECTION)\\n\")\n",
    "        f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"EXECUTIVE SUMMARY\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        f.write(f\"This analysis compares AttBiLSTM model performance across \")\n",
    "        f.write(f\"different dataset sizes: {', '.join(map(str, DATASET_SIZES))} samples.\\n\")\n",
    "        f.write(f\"Dataset used: Combined_Common_Genes_With_Target_ML_Selected_Top_9000_Features.csv\\n\\n\")\n",
    "        \n",
    "        # Find best performing size for each metric\n",
    "        best_performers = {}\n",
    "        for metric in ['ACC', 'AUC', 'F1', 'MCC']:\n",
    "            best_size = max(DATASET_SIZES, key=lambda x: all_metrics[x][metric])\n",
    "            best_value = all_metrics[best_size][metric]\n",
    "            best_performers[metric] = (best_size, best_value)\n",
    "        \n",
    "        f.write(\"BEST PERFORMING DATASET SIZES:\\n\")\n",
    "        for metric, (size, value) in best_performers.items():\n",
    "            f.write(f\"  {metric}: {size} samples ({value:.4f})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        # Performance trends\n",
    "        f.write(\"PERFORMANCE TRENDS:\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        for metric in ['ACC', 'AUC', 'F1', 'MCC']:\n",
    "            values = [all_metrics[size][metric] for size in DATASET_SIZES]\n",
    "            trend = \"Increasing\" if values[-1] > values[0] else \"Decreasing\"\n",
    "            improvement = ((values[-1] - values[0]) / values[0]) * 100\n",
    "            f.write(f\"{metric}: {trend} trend ({improvement:+.2f}% change from 6K to 9K samples)\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        # Training efficiency\n",
    "        f.write(\"TRAINING EFFICIENCY:\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        training_times = [all_metrics[size]['Training_Time'] for size in DATASET_SIZES]\n",
    "        f.write(f\"Training time range: {min(training_times):.1f}s - {max(training_times):.1f}s\\n\")\n",
    "        f.write(f\"Time increase from 6K to 9K: {((training_times[-1] - training_times[0]) / training_times[0]) * 100:.1f}%\\n\\n\")\n",
    "        \n",
    "        # Detailed metrics table\n",
    "        f.write(\"DETAILED METRICS TABLE:\\n\")\n",
    "        f.write(\"-\" * 25 + \"\\n\")\n",
    "        f.write(summary_df.round(4).to_string(index=False))\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        # Recommendations\n",
    "        f.write(\"RECOMMENDATIONS:\\n\")\n",
    "        f.write(\"-\" * 15 + \"\\n\")\n",
    "        \n",
    "        # Find most balanced performer (considering accuracy, F1, and training time)\n",
    "        balanced_scores = []\n",
    "        for size in DATASET_SIZES:\n",
    "            metrics = all_metrics[size]\n",
    "            # Normalize metrics (higher is better for performance, lower is better for time)\n",
    "            norm_acc = metrics['ACC']\n",
    "            norm_f1 = metrics['F1']\n",
    "            norm_time = 1 / (metrics['Training_Time'] / min(training_times))  # Inverse for time\n",
    "            balanced_score = (norm_acc + norm_f1 + norm_time) / 3\n",
    "            balanced_scores.append((size, balanced_score))\n",
    "        \n",
    "        best_balanced = max(balanced_scores, key=lambda x: x[1])\n",
    "        f.write(f\"1. Most balanced performance: {best_balanced[0]} samples\\n\")\n",
    "        f.write(f\"   (Considering accuracy, F1-score, and training efficiency)\\n\\n\")\n",
    "        \n",
    "        f.write(f\"2. For maximum accuracy: {best_performers['ACC'][0]} samples \")\n",
    "        f.write(f\"({best_performers['ACC'][1]:.4f})\\n\\n\")\n",
    "        \n",
    "        f.write(f\"3. For best F1-score: {best_performers['F1'][0]} samples \")\n",
    "        f.write(f\"({best_performers['F1'][1]:.4f})\\n\\n\")\n",
    "        \n",
    "        # Sample efficiency analysis\n",
    "        sample_efficiency = []\n",
    "        for i, size in enumerate(DATASET_SIZES):\n",
    "            if i > 0:\n",
    "                prev_size = DATASET_SIZES[i-1]\n",
    "                acc_gain = all_metrics[size]['ACC'] - all_metrics[prev_size]['ACC']\n",
    "                sample_gain = size - prev_size\n",
    "                efficiency = acc_gain / sample_gain * 1000  # Per 1000 samples\n",
    "                sample_efficiency.append((size, efficiency))\n",
    "        \n",
    "        f.write(\"SAMPLE EFFICIENCY ANALYSIS:\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        f.write(\"Accuracy gain per 1000 additional samples:\\n\")\n",
    "        for size, efficiency in sample_efficiency:\n",
    "            f.write(f\"  Up to {size} samples: {efficiency:.6f}\\n\")\n",
    "        \n",
    "        if sample_efficiency:\n",
    "            best_efficiency = max(sample_efficiency, key=lambda x: x[1])\n",
    "            f.write(f\"\\nMost efficient range: Up to {best_efficiency[0]} samples\\n\")\n",
    "    \n",
    "    print(f\"Comprehensive summary saved to: {summary_path}\")\n",
    "    print(f\"Detailed report saved to: {report_path}\")\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ecd85cfb16ac8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T03:49:07.915552Z",
     "start_time": "2025-06-03T03:49:07.908431Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_complete_dataset_size_comparison():\n",
    "    \"\"\"Run the complete dataset size comparison analysis\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"ADVANCED ATTBILSTM DATASET SIZE COMPARISON ANALYSIS (WITH FS)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Comparing performance across dataset sizes: {DATASET_SIZES}\")\n",
    "    print(f\"Full dataset size: {len(X_full)} samples\")\n",
    "    print(f\"Number of features: {num_features}\")\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    \n",
    "    # Store results for all dataset sizes\n",
    "    all_metrics = {}\n",
    "    all_histories = {}\n",
    "    all_test_data = {}\n",
    "    \n",
    "    # Run analysis for each dataset size\n",
    "    for dataset_size in DATASET_SIZES:\n",
    "        try:\n",
    "            metrics, history, roc_data = run_analysis_for_dataset_size(dataset_size)\n",
    "            all_metrics[dataset_size] = metrics\n",
    "            all_histories[dataset_size] = history\n",
    "            all_test_data[dataset_size] = roc_data\n",
    "            \n",
    "            # Memory cleanup after each run\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            elif torch.backends.mps.is_available():\n",
    "                torch.mps.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing dataset size {dataset_size}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Generate comparative analysis\n",
    "    if len(all_metrics) > 1:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"GENERATING COMPARATIVE ANALYSIS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Create comprehensive comparative plots\n",
    "        plot_comparative_results(all_metrics, all_histories, all_test_data)\n",
    "        \n",
    "        # Create comprehensive summary\n",
    "        summary_df = create_comprehensive_summary(all_metrics)\n",
    "        \n",
    "        # Print final comparison\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FINAL COMPARISON SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        comparison_metrics = ['ACC', 'AUC', 'F1', 'MCC', 'Training_Time']\n",
    "        print(f\"{'Size':<8}\", end=\"\")\n",
    "        for metric in comparison_metrics:\n",
    "            print(f\"{metric:<12}\", end=\"\")\n",
    "        print()\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for size in DATASET_SIZES:\n",
    "            if size in all_metrics:\n",
    "                metrics = all_metrics[size]\n",
    "                print(f\"{size:<8}\", end=\"\")\n",
    "                for metric in comparison_metrics:\n",
    "                    if metric == 'Training_Time':\n",
    "                        print(f\"{metrics[metric]:<12.1f}\", end=\"\")\n",
    "                    else:\n",
    "                        print(f\"{metrics[metric]:<12.4f}\", end=\"\")\n",
    "                print()\n",
    "        \n",
    "        # Find overall best performer\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"KEY FINDINGS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        best_acc = max(DATASET_SIZES, key=lambda x: all_metrics.get(x, {}).get('ACC', 0))\n",
    "        best_f1 = max(DATASET_SIZES, key=lambda x: all_metrics.get(x, {}).get('F1', 0))\n",
    "        fastest = min(DATASET_SIZES, key=lambda x: all_metrics.get(x, {}).get('Training_Time', float('inf')))\n",
    "        \n",
    "        print(f\"🏆 Best Accuracy: {best_acc} samples ({all_metrics[best_acc]['ACC']:.4f})\")\n",
    "        print(f\"🎯 Best F1-Score: {best_f1} samples ({all_metrics[best_f1]['F1']:.4f})\")\n",
    "        print(f\"⚡ Fastest Training: {fastest} samples ({all_metrics[fastest]['Training_Time']:.1f}s)\")\n",
    "        \n",
    "        # Performance improvement analysis\n",
    "        first_size = min(DATASET_SIZES)\n",
    "        last_size = max(DATASET_SIZES)\n",
    "        if first_size in all_metrics and last_size in all_metrics:\n",
    "            acc_improvement = ((all_metrics[last_size]['ACC'] - all_metrics[first_size]['ACC']) / \n",
    "                             all_metrics[first_size]['ACC']) * 100\n",
    "            time_increase = ((all_metrics[last_size]['Training_Time'] - all_metrics[first_size]['Training_Time']) / \n",
    "                           all_metrics[first_size]['Training_Time']) * 100\n",
    "            \n",
    "            print(f\"\\n📈 Performance change from {first_size} to {last_size} samples:\")\n",
    "            print(f\"   Accuracy: {acc_improvement:+.2f}%\")\n",
    "            print(f\"   Training time: {time_increase:+.1f}%\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"All results saved in directory: {BASE_DIR}\")\n",
    "    print(\"\\nGenerated files:\")\n",
    "    print(\"📁 Models saved in: AttBiLSTM_Analysis_With_FS/models/\")\n",
    "    print(\"   - attbilstm_model_6000.pth\")\n",
    "    print(\"   - attbilstm_model_7000.pth\") \n",
    "    print(\"   - attbilstm_model_8000.pth\")\n",
    "    print(\"   - attbilstm_model_9000.pth\")\n",
    "    print(\"📊 All plots saved in: AttBiLSTM_Analysis_With_FS/plots/\")\n",
    "    print(\"   - combined_roc_curves.png/pdf\")\n",
    "    print(\"   - training_loss_comparison.png/pdf\")\n",
    "    print(\"   - validation_loss_comparison.png/pdf\")\n",
    "    print(\"   - metrics_comparison_bar.png/pdf\")\n",
    "    print(\"   - metrics_comparison_line.png/pdf\")\n",
    "    print(\"   - metrics_comparison_violin.png/pdf\")\n",
    "    print(\"   - metrics_comparison_radar.png/pdf\")\n",
    "    print(\"   - training_time_horizontal.png/pdf\")\n",
    "    print(\"   - testing_time_horizontal.png/pdf\")\n",
    "    print(\"📋 All results saved in: AttBiLSTM_Analysis_With_FS/results/\")\n",
    "    print(\"   - metrics_XXXX.csv (for each dataset size)\")\n",
    "    print(\"   - training_history_XXXX.csv (for each dataset size)\")\n",
    "    print(\"   - comprehensive_summary.csv\")\n",
    "    print(\"   - analysis_report.txt\")\n",
    "    \n",
    "    return all_metrics, all_histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1ab5f4d792a8c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T03:55:02.591118Z",
     "start_time": "2025-06-03T03:49:07.923638Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        all_metrics, all_histories = run_complete_dataset_size_comparison()\n",
    "        \n",
    "        # Final memory cleanup\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        elif torch.backends.mps.is_available():\n",
    "            torch.mps.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        print(\"\\n✅ Dataset size comparison analysis completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during analysis: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    finally:\n",
    "        print(\"🏁 Analysis pipeline finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
