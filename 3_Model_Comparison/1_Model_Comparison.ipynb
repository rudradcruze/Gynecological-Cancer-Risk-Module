{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import pandas as pd\n",
   "id": "8724c6b752d87202",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = pd.read_csv('../dataset/FINAL_DATASET_2000.csv')\n",
    "\n",
    "data.info()"
   ],
   "id": "bbd4824fd33b909e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, precision_score, f1_score, matthews_corrcoef, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F"
   ],
   "id": "ed1180d85bf1a6ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set environment variables for stability\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "\n",
    "# Set font family globally\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "# Base directory\n",
    "BASE_DIR = \"Results\"\n",
    "os.makedirs(BASE_DIR, exist_ok=True)"
   ],
   "id": "14a6e8ebb7d34e8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Device setup\n",
    "device = torch.device(\"cpu\")\n",
    "try:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"MPS device detected, using MPS\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"CUDA device detected, using CUDA\")\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "    print(f\"Device: {device}\")\n",
    "except:\n",
    "    print(\"Using CPU (default)\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING AND PREPARING DATA\")\n",
    "print(\"=\" * 70)"
   ],
   "id": "3fd473edac64bee9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv('../dataset/Combined_Common_Genes_With_Target_ML.csv')\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Columns: {list(data.columns[-5:])}\")  # Show last 5 columns to confirm 'target'\n",
    "\n",
    "# Prepare features and target\n",
    "X = data.drop('target', axis=1).values\n",
    "y = data['target'].values\n",
    "\n",
    "# Ensure y is 1D\n",
    "y = y.ravel()"
   ],
   "id": "22675e9b4dccba56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Classes: {np.unique(y)}\")\n",
    "print(f\"Class distribution: {np.bincount(y) / len(y)}\")\n",
    "\n",
    "num_classes = len(np.unique(y))\n",
    "num_features = X.shape[1]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42,\n",
    "                                                          stratify=y_train)\n",
    "\n",
    "print(f\"Train subset: {X_train_sub.shape}, {y_train_sub.shape}\")\n",
    "print(f\"Validation: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test: {X_test.shape}, {y_test.shape}\")"
   ],
   "id": "2090413f1a04c963",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ==================== NEURAL NETWORK MODELS ====================\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes, input_features):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(32, 16, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Calculate output size after pooling operations\n",
    "        conv_output_size = input_features // 8  # After 3 pooling operations\n",
    "        self.fc1 = nn.Linear(16 * conv_output_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # [batch, 1, features]\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "id": "b814dc03800870dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# SIMPLE BUT IMPROVED AttBiLSTM\n",
    "class SimpleImprovedAttBiLSTM(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleImprovedAttBiLSTM, self).__init__()\n",
    "\n",
    "        # Simple but effective LSTM\n",
    "        self.lstm = nn.LSTM(1, 128, num_layers=2, bidirectional=True,\n",
    "                            batch_first=True, dropout=0.3)\n",
    "\n",
    "        # Simple attention\n",
    "        self.attention = nn.Linear(256, 1)  # 128*2 for bidirectional\n",
    "\n",
    "        # Simple but effective classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(x)  # [batch, seq_len, 256]\n",
    "\n",
    "        # Simple attention\n",
    "        attn_weights = F.softmax(self.attention(lstm_out), dim=1)  # [batch, seq_len, 1]\n",
    "        context = torch.sum(lstm_out * attn_weights, dim=1)  # [batch, 256]\n",
    "\n",
    "        # Classification\n",
    "        output = self.classifier(context)\n",
    "        return output"
   ],
   "id": "50a97c71114df08b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "class FixedAttBiLSTM(nn.Module):\n",
    "    def __init__(self, num_classes, input_features):\n",
    "        super(FixedAttBiLSTM, self).__init__()\n",
    "\n",
    "        self.input_features = input_features\n",
    "\n",
    "        # Input projection to reduce dimensionality and add non-linearity\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # BiLSTM layers\n",
    "        self.lstm1 = nn.LSTM(64, 128, batch_first=True, bidirectional=True, dropout=0.3)\n",
    "        self.lstm2 = nn.LSTM(256, 64, batch_first=True, bidirectional=True, dropout=0.3)\n",
    "\n",
    "        # Improved attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                if len(param.shape) >= 2:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                else:\n",
    "                    nn.init.uniform_(param, -0.1, 0.1)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # Project input features\n",
    "        x = self.input_projection(x)  # [batch, seq_len, 64]\n",
    "\n",
    "        # First LSTM layer\n",
    "        lstm_out1, _ = self.lstm1(x)  # [batch, seq_len, 256]\n",
    "\n",
    "        # Second LSTM layer\n",
    "        lstm_out2, _ = self.lstm2(lstm_out1)  # [batch, seq_len, 128]\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention_weights = self.attention(lstm_out2)  # [batch, seq_len, 1]\n",
    "\n",
    "        # Apply attention\n",
    "        context_vector = torch.sum(lstm_out2 * attention_weights, dim=1)  # [batch, 128]\n",
    "\n",
    "        # Classification\n",
    "        output = self.classifier(context_vector)\n",
    "\n",
    "        return output"
   ],
   "id": "8dce8ff5dc2be0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_metrics(y_true, y_pred, y_prob=None):\n",
    "    \"\"\"Calculate all evaluation metrics\"\"\"\n",
    "    # Basic metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "    # Confusion matrix components for binary classification\n",
    "    if num_classes == 2:\n",
    "        tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "        tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "        fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    else:\n",
    "        # For multiclass, calculate average\n",
    "        specificities = []\n",
    "        sensitivities = []\n",
    "        for class_i in range(num_classes):\n",
    "            tp = np.sum((y_true == class_i) & (y_pred == class_i))\n",
    "            tn = np.sum((y_true != class_i) & (y_pred != class_i))\n",
    "            fp = np.sum((y_true != class_i) & (y_pred == class_i))\n",
    "            fn = np.sum((y_true == class_i) & (y_pred != class_i))\n",
    "\n",
    "            spec = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "            sens = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "            specificities.append(spec)\n",
    "            sensitivities.append(sens)\n",
    "\n",
    "        specificity = np.mean(specificities)\n",
    "        sensitivity = np.mean(sensitivities)\n",
    "\n",
    "    results = {\n",
    "        'ACC': acc,\n",
    "        'PRE': precision,\n",
    "        'SP': specificity,\n",
    "        'SN': sensitivity,\n",
    "        'F1': f1,\n",
    "        'MCC': mcc\n",
    "    }\n",
    "\n",
    "    # Calculate AUC if probabilities provided\n",
    "    if y_prob is not None:\n",
    "        if num_classes == 2:\n",
    "            fpr, tpr, _ = roc_curve(y_true, y_prob[:, 1])\n",
    "            results['AUC'] = auc(fpr, tpr)\n",
    "            roc_data = [(fpr, tpr, results['AUC'])]\n",
    "        else:\n",
    "            aucs = []\n",
    "            roc_data = []\n",
    "            for i in range(num_classes):\n",
    "                fpr, tpr, _ = roc_curve(y_true == i, y_prob[:, i])\n",
    "                class_auc = auc(fpr, tpr)\n",
    "                aucs.append(class_auc)\n",
    "                roc_data.append((fpr, tpr, class_auc))\n",
    "            results['AUC'] = np.mean(aucs)\n",
    "    else:\n",
    "        results['AUC'] = 0.0\n",
    "        roc_data = []\n",
    "\n",
    "    return results, roc_data"
   ],
   "id": "e31a4450843a2c73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def fixed_train_attbilstm(model, X_train, y_train, X_val, y_val, epochs=80, batch_size=32):\n",
    "    \"\"\"Fixed training function for AttBiLSTM\"\"\"\n",
    "    print(\"Training Fixed AttBiLSTM...\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Normalize data - CRITICAL for LSTM\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    # Reshape for LSTM: treat each feature as a time step\n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled).unsqueeze(-1)  # [batch, features, 1]\n",
    "    X_val_tensor = torch.FloatTensor(X_val_scaled).unsqueeze(-1)\n",
    "    y_train_tensor = torch.LongTensor(y_train)\n",
    "    y_val_tensor = torch.LongTensor(y_val)\n",
    "\n",
    "    # Data loaders\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, drop_last=False)\n",
    "\n",
    "    # Improved optimizer setup\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Label smoothing for better generalization\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.002, weight_decay=1e-4)  # AdamW instead of Adam\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "    # Early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_acc = 0.0\n",
    "    patience = 20\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    print(f\"Starting training for {epochs} epochs...\")\n",
    "    print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # Backward pass with gradient clipping\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += y_batch.size(0)\n",
    "            train_correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "            # Print progress every 10 batches\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, \"\n",
    "                      f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += y_batch.size(0)\n",
    "                val_correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "        # Calculate metrics\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        train_acc = 100.0 * train_correct / train_total\n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"  Learning Rate: {current_lr:.6f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # Early stopping based on validation loss and accuracy\n",
    "        if avg_val_loss < best_val_loss or val_acc > best_val_acc:\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            print(f\"  >>> New best model! Val Loss: {best_val_loss:.4f}, Val Acc: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement. Patience: {patience_counter}/{patience}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"Loaded best model with Val Loss: {best_val_loss:.4f}, Val Acc: {best_val_acc:.2f}%\")\n",
    "\n",
    "    return model, scaler"
   ],
   "id": "a41568c592e09418",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def fixed_evaluate_attbilstm(model, scaler, X_test, y_test, batch_size=32):\n",
    "    \"\"\"Fixed evaluation for AttBiLSTM\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Apply same scaling and reshaping\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled).unsqueeze(-1)\n",
    "    test_dataset = TensorDataset(X_test_tensor, torch.LongTensor(y_test))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "\n",
    "            # Get probabilities and predictions\n",
    "            probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "            all_probs.append(probs)\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "\n",
    "    test_time = time.time() - start_time\n",
    "\n",
    "    # Combine results\n",
    "    all_probs = np.vstack(all_probs)\n",
    "    predicted_classes = np.argmax(all_probs, axis=1)\n",
    "\n",
    "    return predicted_classes, all_probs, test_time"
   ],
   "id": "1601526762f44bb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Updated training function to use the fixed version\n",
    "def train_fixed_attbilstm():\n",
    "    \"\"\"Train Fixed AttBiLSTM\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"TRAINING FIXED ATTBILSTM\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create fixed model\n",
    "    bilstm_model = FixedAttBiLSTM(num_classes, num_features)\n",
    "\n",
    "    # Train with fixed function\n",
    "    bilstm_model, scaler = fixed_train_attbilstm(\n",
    "        bilstm_model, X_train_sub, y_train_sub, X_val, y_val,\n",
    "        epochs=80, batch_size=32\n",
    "    )\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred, y_prob, test_time = fixed_evaluate_attbilstm(\n",
    "        bilstm_model, scaler, X_test, y_test\n",
    "    )\n",
    "\n",
    "    results, roc_data = calculate_metrics(y_test, y_pred, y_prob)\n",
    "    results['Training Time'] = train_time\n",
    "    results['Testing Time'] = test_time\n",
    "\n",
    "    print(f\"Training time: {train_time:.4f}s\")\n",
    "    print(f\"Testing time: {test_time:.4f}s\")\n",
    "    print(f\"Accuracy: {results['ACC']:.4f}\")\n",
    "    print(f\"AUC: {results['AUC']:.4f}\")\n",
    "    print(f\"F1-Score: {results['F1']:.4f}\")\n",
    "    print(f\"MCC: {results['MCC']:.4f}\")\n",
    "\n",
    "    # Cleanup\n",
    "    del bilstm_model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    elif torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return results, roc_data"
   ],
   "id": "74e0dd14d3ed9e63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_pytorch_model(model, X_train, y_train, X_val, y_val, epochs=50, batch_size=32):\n",
    "    \"\"\"Train PyTorch models (for CNN)\"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    # Create data loaders\n",
    "    X_train_tensor = torch.FloatTensor(X_train).unsqueeze(-1)\n",
    "    X_val_tensor = torch.FloatTensor(X_val).unsqueeze(-1)\n",
    "    y_train_tensor = torch.LongTensor(y_train)\n",
    "    y_val_tensor = torch.LongTensor(y_val)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Training setup\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                val_loss += criterion(outputs, y_batch).item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{epochs} - Train Loss: {train_loss / len(train_loader):.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return model"
   ],
   "id": "14d5ffeebcbfce98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def simple_train_attbilstm(model, X_train, y_train, X_val, y_val, epochs=60, batch_size=64):\n",
    "    \"\"\"Simple but effective training for AttBiLSTM\"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    # MOST IMPORTANT: Normalize the data!\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    # Create tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled).unsqueeze(-1)\n",
    "    X_val_tensor = torch.FloatTensor(X_val_scaled).unsqueeze(-1)\n",
    "    y_train_tensor = torch.LongTensor(y_train)\n",
    "    y_val_tensor = torch.LongTensor(y_val)\n",
    "\n",
    "    # Data loaders\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Simple but effective training setup\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "    # Early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 15\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    print(f\"Training Simple AttBiLSTM for {epochs} epochs...\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                val_loss += criterion(outputs, y_batch).item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model, scaler"
   ],
   "id": "74df86403f3e3921",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_pytorch_model(model, X_test, y_test, batch_size=32):\n",
    "    \"\"\"Evaluate PyTorch models (for CNN)\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    X_test_tensor = torch.FloatTensor(X_test).unsqueeze(-1)\n",
    "    test_dataset = TensorDataset(X_test_tensor, torch.LongTensor(y_test))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    all_preds = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch).cpu().numpy()\n",
    "            all_preds.append(outputs)\n",
    "\n",
    "    test_time = time.time() - start_time\n",
    "\n",
    "    predictions = np.vstack(all_preds)\n",
    "    probabilities = torch.softmax(torch.tensor(predictions), dim=1).numpy()\n",
    "    predicted_classes = np.argmax(probabilities, axis=1)\n",
    "\n",
    "    return predicted_classes, probabilities, test_time"
   ],
   "id": "9853058521cfe16b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def simple_evaluate_attbilstm(model, scaler, X_test, y_test, batch_size=64):\n",
    "    \"\"\"Simple evaluation for AttBiLSTM\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Apply same scaling\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled).unsqueeze(-1)\n",
    "    test_dataset = TensorDataset(X_test_tensor, torch.LongTensor(y_test))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    all_preds = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch).cpu().numpy()\n",
    "            all_preds.append(outputs)\n",
    "\n",
    "    test_time = time.time() - start_time\n",
    "\n",
    "    predictions = np.vstack(all_preds)\n",
    "    probabilities = torch.softmax(torch.tensor(predictions), dim=1).numpy()\n",
    "    predicted_classes = np.argmax(probabilities, axis=1)\n",
    "\n",
    "    return predicted_classes, probabilities, test_time"
   ],
   "id": "9df716e3d7066cf2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_random_forest():\n",
    "    \"\"\"Train Random Forest\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"TRAINING RANDOM FOREST\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    start_time = time.time()\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_model.fit(X_train_sub, y_train_sub)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    y_prob = rf_model.predict_proba(X_test)\n",
    "    test_time = time.time() - start_time\n",
    "\n",
    "    results, roc_data = calculate_metrics(y_test, y_pred, y_prob)\n",
    "    results['Training Time'] = train_time\n",
    "    results['Testing Time'] = test_time\n",
    "\n",
    "    print(f\"Training time: {train_time:.4f}s\")\n",
    "    print(f\"Testing time: {test_time:.4f}s\")\n",
    "    print(f\"Accuracy: {results['ACC']:.4f}\")\n",
    "    print(f\"AUC: {results['AUC']:.4f}\")\n",
    "\n",
    "    return results, roc_data"
   ],
   "id": "e17b1a26c282ba97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_svm():\n",
    "    \"\"\"Train SVM\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"TRAINING SVM\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    start_time = time.time()\n",
    "    svm_model = SVC(\n",
    "        kernel='rbf',\n",
    "        C=1.0,\n",
    "        probability=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    svm_model.fit(X_train_sub, y_train_sub)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    y_pred = svm_model.predict(X_test)\n",
    "    y_prob = svm_model.predict_proba(X_test)\n",
    "    test_time = time.time() - start_time\n",
    "\n",
    "    results, roc_data = calculate_metrics(y_test, y_pred, y_prob)\n",
    "    results['Training Time'] = train_time\n",
    "    results['Testing Time'] = test_time\n",
    "\n",
    "    print(f\"Training time: {train_time:.4f}s\")\n",
    "    print(f\"Testing time: {test_time:.4f}s\")\n",
    "    print(f\"Accuracy: {results['ACC']:.4f}\")\n",
    "    print(f\"AUC: {results['AUC']:.4f}\")\n",
    "\n",
    "    return results, roc_data"
   ],
   "id": "5efbbcb49b31e774",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_xgboost():\n",
    "    \"\"\"Train XGBoost\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"TRAINING XGBOOST\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    try:\n",
    "        # Force CPU usage for XGBoost to avoid MPS conflicts\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Use XGBClassifier instead of xgb.train for better compatibility\n",
    "        if num_classes == 2:\n",
    "            xgb_model = xgb.XGBClassifier(\n",
    "                objective='binary:logistic',\n",
    "                eval_metric='logloss',\n",
    "                max_depth=6,\n",
    "                learning_rate=0.1,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                n_estimators=100,\n",
    "                early_stopping_rounds=10,\n",
    "                random_state=42,\n",
    "                verbosity=0,\n",
    "                tree_method='hist',  # Use histogram method for stability\n",
    "                device='cpu'  # Force CPU\n",
    "            )\n",
    "        else:\n",
    "            xgb_model = xgb.XGBClassifier(\n",
    "                objective='multi:softprob',\n",
    "                eval_metric='mlogloss',\n",
    "                max_depth=6,\n",
    "                learning_rate=0.1,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                n_estimators=100,\n",
    "                early_stopping_rounds=10,\n",
    "                random_state=42,\n",
    "                verbosity=0,\n",
    "                tree_method='hist',\n",
    "                device='cpu'\n",
    "            )\n",
    "\n",
    "        # Fit with evaluation set\n",
    "        xgb_model.fit(\n",
    "            X_train_sub, y_train_sub,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "        train_time = time.time() - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        y_prob = xgb_model.predict_proba(X_test)\n",
    "        test_time = time.time() - start_time\n",
    "\n",
    "        results, roc_data = calculate_metrics(y_test, y_pred, y_prob)\n",
    "        results['Training Time'] = train_time\n",
    "        results['Testing Time'] = test_time\n",
    "\n",
    "        print(f\"Training time: {train_time:.4f}s\")\n",
    "        print(f\"Testing time: {test_time:.4f}s\")\n",
    "        print(f\"Accuracy: {results['ACC']:.4f}\")\n",
    "        print(f\"AUC: {results['AUC']:.4f}\")\n",
    "\n",
    "        return results, roc_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"XGBoost training failed: {e}\")\n",
    "        print(\"Attempting fallback with basic parameters...\")\n",
    "\n",
    "        try:\n",
    "            # Fallback with minimal parameters\n",
    "            start_time = time.time()\n",
    "            xgb_model = xgb.XGBClassifier(\n",
    "                n_estimators=50,\n",
    "                max_depth=3,\n",
    "                learning_rate=0.1,\n",
    "                random_state=42,\n",
    "                verbosity=0,\n",
    "                device='cpu'\n",
    "            )\n",
    "            xgb_model.fit(X_train_sub, y_train_sub)\n",
    "            train_time = time.time() - start_time\n",
    "\n",
    "            start_time = time.time()\n",
    "            y_pred = xgb_model.predict(X_test)\n",
    "            y_prob = xgb_model.predict_proba(X_test)\n",
    "            test_time = time.time() - start_time\n",
    "\n",
    "            results, roc_data = calculate_metrics(y_test, y_pred, y_prob)\n",
    "            results['Training Time'] = train_time\n",
    "            results['Testing Time'] = test_time\n",
    "\n",
    "            print(f\"Fallback successful!\")\n",
    "            print(f\"Training time: {train_time:.4f}s\")\n",
    "            print(f\"Testing time: {test_time:.4f}s\")\n",
    "            print(f\"Accuracy: {results['ACC']:.4f}\")\n",
    "            print(f\"AUC: {results['AUC']:.4f}\")\n",
    "\n",
    "            return results, roc_data\n",
    "\n",
    "        except Exception as e2:\n",
    "            print(f\"XGBoost fallback also failed: {e2}\")\n",
    "            return {\n",
    "                'ACC': 0.0, 'AUC': 0.0, 'PRE': 0.0, 'SP': 0.0, 'SN': 0.0,\n",
    "                'F1': 0.0, 'MCC': 0.0, 'Training Time': 0.0, 'Testing Time': 0.0,\n",
    "                'Error': str(e2)\n",
    "            }, []"
   ],
   "id": "9069775a3f6e8ab6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_cnn():\n",
    "    \"\"\"Train CNN\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"TRAINING CNN\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    start_time = time.time()\n",
    "    cnn_model = CNN(num_classes, num_features)\n",
    "    cnn_model = train_pytorch_model(cnn_model, X_train_sub, y_train_sub, X_val, y_val, epochs=50)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    y_pred, y_prob, test_time = evaluate_pytorch_model(cnn_model, X_test, y_test)\n",
    "\n",
    "    results, roc_data = calculate_metrics(y_test, y_pred, y_prob)\n",
    "    results['Training Time'] = train_time\n",
    "    results['Testing Time'] = test_time\n",
    "\n",
    "    print(f\"Training time: {train_time:.4f}s\")\n",
    "    print(f\"Testing time: {test_time:.4f}s\")\n",
    "    print(f\"Accuracy: {results['ACC']:.4f}\")\n",
    "    print(f\"AUC: {results['AUC']:.4f}\")\n",
    "\n",
    "    # Cleanup\n",
    "    del cnn_model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    elif torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return results, roc_data"
   ],
   "id": "4e9411700ad32bea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_attbilstm():\n",
    "    \"\"\"Train Simple Improved AttBiLSTM\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"TRAINING SIMPLE IMPROVED ATTBILSTM\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Simple but improved model\n",
    "    bilstm_model = SimpleImprovedAttBiLSTM(num_classes)\n",
    "\n",
    "    # Simple but effective training\n",
    "    bilstm_model, scaler = simple_train_attbilstm(\n",
    "        bilstm_model, X_train_sub, y_train_sub, X_val, y_val,\n",
    "        epochs=100, batch_size=64\n",
    "    )\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    # Simple evaluation\n",
    "    y_pred, y_prob, test_time = simple_evaluate_attbilstm(\n",
    "        bilstm_model, scaler, X_test, y_test\n",
    "    )\n",
    "\n",
    "    results, roc_data = calculate_metrics(y_test, y_pred, y_prob)\n",
    "    results['Training Time'] = train_time\n",
    "    results['Testing Time'] = test_time\n",
    "\n",
    "    print(f\"Training time: {train_time:.4f}s\")\n",
    "    print(f\"Testing time: {test_time:.4f}s\")\n",
    "    print(f\"Accuracy: {results['ACC']:.4f}\")\n",
    "    print(f\"AUC: {results['AUC']:.4f}\")\n",
    "    print(f\"F1-Score: {results['F1']:.4f}\")\n",
    "\n",
    "    # Cleanup\n",
    "    del bilstm_model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    elif torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return results, roc_data"
   ],
   "id": "f37363f4f5bcf37a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STARTING 5-MODEL COMPARISON WITH SIMPLE IMPROVED ATTBILSTM\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_results = {}\n",
    "all_roc_data = {}\n",
    "\n",
    "# Define models to run\n",
    "models = [\n",
    "    (\"RF\", train_random_forest),\n",
    "    (\"SVM\", train_svm),\n",
    "    (\"XGBoost\", train_xgboost),\n",
    "    (\"CNN\", train_cnn),\n",
    "    (\"Fixed_AttBiLSTM\", train_fixed_attbilstm)\n",
    "]\n",
    "\n",
    "# Train all models\n",
    "for model_name, train_func in models:\n",
    "    try:\n",
    "        print(f\"\\nStarting {model_name}...\")\n",
    "\n",
    "        # Add memory cleanup before each model\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        elif torch.backends.mps.is_available():\n",
    "            torch.mps.empty_cache()\n",
    "\n",
    "        results, roc_data = train_func()\n",
    "        all_results[model_name] = results\n",
    "        all_roc_data[model_name] = roc_data\n",
    "\n",
    "        print(f\"{model_name} completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error training {model_name}: {e}\")\n",
    "        print(f\"Continuing with next model...\")\n",
    "        all_results[model_name] = {\n",
    "            'ACC': 0.0, 'AUC': 0.0, 'PRE': 0.0, 'SP': 0.0, 'SN': 0.0,\n",
    "            'F1': 0.0, 'MCC': 0.0, 'Training Time': 0.0, 'Testing Time': 0.0,\n",
    "            'Error': str(e)\n",
    "        }\n",
    "        all_roc_data[model_name] = []\n",
    "\n",
    "# Create results summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results_df = pd.DataFrame(all_results).T\n",
    "column_order = ['ACC', 'AUC', 'PRE', 'SP', 'SN', 'F1', 'MCC', 'Training Time', 'Testing Time']\n",
    "results_df = results_df.reindex(columns=column_order)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(os.path.join(BASE_DIR, 'simple_improved_model_comparison_results.csv'))\n",
    "print(\"\\nResults Table:\")\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "\n",
    "for i, (model_name, roc_data) in enumerate(all_roc_data.items()):\n",
    "    if roc_data and len(roc_data) > 0:\n",
    "        # Use first class ROC for display (or the only one for binary)\n",
    "        fpr, tpr, auc_value = roc_data[0]\n",
    "        label_name = model_name.replace('_', ' ')\n",
    "        plt.plot(fpr, tpr, color=colors[i],\n",
    "                 label=f'{label_name} (AUC = {auc_value:.3f})', linewidth=3)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
    "\n",
    "# Set plot styling\n",
    "ax = plt.gca()\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_linewidth(2)\n",
    "    spine.set_color('black')\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=24)\n",
    "plt.ylabel('True Positive Rate', fontsize=24)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.title('ROC Curves - Simple Improved AttBiLSTM vs Other Models', fontsize=28, pad=20)\n",
    "plt.legend(loc='lower right', fontsize=18)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(os.path.join(BASE_DIR, 'simple_improved_models_roc.png'), dpi=1000)\n",
    "plt.savefig(os.path.join(BASE_DIR, 'simple_improved_models_roc.pdf'), dpi=1000)\n",
    "plt.show()\n",
    "\n",
    "# Model rankings\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"MODEL RANKING BY ACCURACY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "acc_ranking = results_df['ACC'].sort_values(ascending=False)\n",
    "for i, (model, acc) in enumerate(acc_ranking.items(), 1):\n",
    "    if not pd.isna(acc):\n",
    "        print(f\"{i}. {model}: {acc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"MODEL RANKING BY AUC\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "auc_ranking = results_df['AUC'].sort_values(ascending=False)\n",
    "for i, (model, auc_score) in enumerate(auc_ranking.items(), 1):\n",
    "    if not pd.isna(auc_score):\n",
    "        print(f\"{i}. {model}: {auc_score:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"MODEL RANKING BY F1-SCORE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "f1_ranking = results_df['F1'].sort_values(ascending=False)\n",
    "for i, (model, f1_score) in enumerate(f1_ranking.items(), 1):\n",
    "    if not pd.isna(f1_score):\n",
    "        print(f\"{i}. {model}: {f1_score:.4f}\")\n",
    "\n",
    "print(f\"\\nAll results saved to: {BASE_DIR}/\")\n",
    "print(\"Generated files:\")\n",
    "print(\"- simple_improved_model_comparison_results.csv\")\n",
    "print(\"- simple_improved_models_roc.png\")\n",
    "print(\"- simple_improved_models_roc.pdf\")"
   ],
   "id": "5f25f816cab0d6bf",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
